#+startup: beamer
#+OPTIONS: H:3 toc:t num:t
#+LaTeX_CLASS: beamer
#+LaTeX_CLASS_OPTIONS: [presentation]
#+BEAMER_THEME: Bergen
* Theoretical Background
** Overview 
- Feed-forward, supervised learning
- Uses backpropagation algorithms
- coNN - constructive neural network
  - 2 main categories:
    - evolutionary based (what is done at Brock)
    - generally constructive. CCNN is main exemplar of this group
** Overview II   
- creates it's own topology starting with minimal network
  - input and output layers only, as usual connected by weights.
- input of nodes code the problem being presented to the network
- output of nodes code the network's response to the input problem
- typically quickprop is used as the learning rule
** Basic Algorithm  
*** Step 1 Training input weights
A new hidden neuron is added one at a time
Instead of a minimum, we find a maximum: the maximum correlation between the candidate output and the residual error of the networks output.
 
1. generate a population of candidate nodes with randomized input weights 
2. inputs are connected, but not outputs
3. repeat training steps until no correlation improvement
   1. one epoch of the training data is run through 
   2. update input weights using any learning rule, such that correlation is increased
4. candidate with highest correlation is put in the network, others are discarded.

- correlation sign doesn't matter, only magnitude
- the bias unit effectively implements a learnable resting activation level for each hidden and output unit.
- subsequent hidden neurons are attached to previous hidden neurons - this is where the cascade term comes from.
*** Step 2 Training output weights 
1. connect output of new node to output layer nodes
   use randomized weights that have adjusted sign to reduce error 
2. input weights are fixed
3. only output weights are trained using any learning rule, until no improvement in error reduction

- once done the new node is fixed permanently.
- any new training involves adding another new node on it's own downstream layer
* Comparisons 
** Two Problems with Backpropagation
*** The Step Size Problem
- vanilla backpropagation requires small steps for convergence - slow
- we do not have the information to pick an optimal learning rate, manually selected
*** The Moving Target Problem
- complication when many factors changing at the same time
- error signal defines problem unit trying to solve, but this keeps changing
- dramatic slowdown of training with increasing number of hidden layers
- herd effect:
  - 2 tasks A, B. If A has bigger effect, all nodes redundantly train for A, ignoring B
  - But when all nodes move toward B at once, problem A response becomes worse.
  - eventually nodes split to train for separate problems A and B, but it takes a long time
  - a randomly initialized network prevents nodes from behaving identically, but this tends to dissipate as the network is trained
- One way to combat: allow only a few weights to change while keeping the others constant
** Pros & Cons
Pros
- At elast 10 times faster than standard backpropagation
  - [performanceChart2.png]
- The network determines its own size and topologies
- Incremental/life long learning: new training, new information can be added, with an already trained network
- effectively deals with the step size and moving target problems
  
Cons
- Very susceptible to overfitting

* Applications
** Popularity
  In spite of the many CoNN algorithms surveyed in (Kwok \& Yeung, 1997a), the most popular for regression problems is no doubt the Cascade Correlation algorithm and maybe the second most popular is the DNC...
  The popularity of Cascade Correlation can be attested by the various ways this
  algorithm has inspired new variations and also has been used in the combined
  approaches between learning methods. [Franco et al, 2009] 
